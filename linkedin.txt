Headline Idea: The "Silent Failure" of Small Models

I spent the weekend trying to break "JSON Mode" in open-source models. I found a silent killer.

The Setup: Tested 3 small models (Gemma 2 2B, Gemma 3 4B, Llama 3.2 3B). 42 stress tests using research benchmarks. No fancy chainsâ€”just "Give me this JSON."

The Expectation: "JSON Mode" is a solved problem, right?

The Reality:
- Overall Parse Success: 100% (It looked like JSON)
- Schema Compliance: Only 76% (It wasn't the right JSON)

The Smoking Gun ðŸ”«

Gemma 2 2B was the culprit.
- Compliance: 29%
- Hallucination Rate: 71%

I asked for data:
{"user": {"name": "Alice"}, "settings": {...}}

It gave me the schema:
{"type": "object", "properties": {"user": {...}}}

Valid JSON? Yes. Useless? Absolutely.

The Fix (Architecture > Model)

I didn't swap the model. I changed the System Prompt.

Added explicit negative constraints:
"Output data, not schema definitions."
"Never include 'type', 'required', or 'properties' in output."

The Result:
âœ… Gemma 2 2B Compliance: 29% â†’ 100%
âœ… Hallucination Rate: 71% â†’ 0%

The Takeaway:
1. "Valid JSON" â‰  "Correct Schema"
2. Small models don't fail by crashingâ€”they fail by hallucinating structure
3. You can fix "dumb" model behavior with "smart" guardrails

(Note: These guardrails slightly confused Llama 3.2 on ultra-complex tasksâ€”trade-offs are everywhere.)

This is part of a weekend experiment seriesâ€”testing what actually works vs what we assume.

ðŸ‘‡ Which small model has burned you the most on structured output?